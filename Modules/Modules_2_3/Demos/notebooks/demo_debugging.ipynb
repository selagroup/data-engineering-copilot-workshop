{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22a8efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Spark session initialized successfully!\n",
      "Spark version: 3.4.1\n",
      "Running on: win32\n",
      "‚úÖ Parsed JDBC connection successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded raw.customers (rows=1000)\n",
      "‚úÖ Loaded raw.orders (rows=5000)\n",
      "‚úÖ Loaded raw.orders (rows=5000)\n",
      "‚úÖ Loaded raw.order_items (rows=15072)\n",
      "‚úÖ Loaded raw.products (rows=200)\n",
      "‚úÖ Loaded raw.products (rows=200)\n",
      "üóÉÔ∏è Cached DataFrame: customers\n",
      "üóÉÔ∏è Cached DataFrame: customers\n",
      "üóÉÔ∏è Cached DataFrame: orders\n",
      "üóÉÔ∏è Cached DataFrame: order_items\n",
      "üóÉÔ∏è Cached DataFrame: products\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install pyspark python-dotenv\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ---- Helper Functions ----\n",
    "\n",
    "def parse_connection(conn_str: str):\n",
    "    \"\"\"Parse and validate a PostgreSQL connection string, returning JDBC URL and credentials.\n",
    "    Raises ValueError with informative message if required components are missing.\"\"\"\n",
    "    if not conn_str:\n",
    "        raise ValueError(\"Empty connection string. Set DB_CONNECTION_STRING in environment.\")\n",
    "    parsed = urlparse(conn_str)\n",
    "    missing = []\n",
    "    if not parsed.hostname: missing.append(\"host\")\n",
    "    if not parsed.port: missing.append(\"port\")\n",
    "    if not parsed.path or parsed.path == '/': missing.append(\"database name\")\n",
    "    if not parsed.username: missing.append(\"username\")\n",
    "    if not parsed.password: missing.append(\"password\")\n",
    "    if missing:\n",
    "        raise ValueError(f\"Connection string missing required components: {', '.join(missing)}\")\n",
    "    # Build JDBC URL with SSL enforced (Azure Postgres typical requirement)\n",
    "    jdbc_url = f\"jdbc:postgresql://{parsed.hostname}:{parsed.port}{parsed.path}?sslmode=require\"\n",
    "    return jdbc_url, parsed.username, parsed.password\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection string (avoid hard-coded credentials; fallback only for demo)\n",
    "DB_CONNECTION = os.getenv(\n",
    "    'DB_CONNECTION_STRING',\n",
    "    'postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db'\n",
    ")\n",
    "\n",
    "# IMPORTANT: Set up Hadoop for Windows BEFORE creating Spark session\n",
    "if sys.platform.startswith('win'):\n",
    "    # Create a minimal Hadoop directory structure for Windows\n",
    "    hadoop_home = os.path.join(os.path.expanduser('~'), '.hadoop')\n",
    "    os.makedirs(hadoop_home, exist_ok=True)\n",
    "    os.makedirs(os.path.join(hadoop_home, 'bin'), exist_ok=True)\n",
    "    os.environ['HADOOP_HOME'] = hadoop_home\n",
    "    \n",
    "    # Download winutils.exe if not present (required for Windows)\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(\"‚ö†Ô∏è winutils.exe not found. Downloading...\")\n",
    "        import urllib.request\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                'https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/winutils.exe',\n",
    "                winutils_path\n",
    "            )\n",
    "            print(\"‚úÖ winutils.exe downloaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not download winutils.exe automatically: {e}\")\n",
    "            print(\"Please download manually from: https://github.com/steveloughran/winutils\")\n",
    "\n",
    "# Initialize Spark Session with PostgreSQL driver\n",
    "# Note: The driver will be downloaded on first run, which may take a moment\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataPipelineDebugging\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Running on: {sys.platform}\")\n",
    "\n",
    "# Attempt to parse connection & report errors clearly\n",
    "try:\n",
    "    JDBC_URL, DB_USER, DB_PASSWORD = parse_connection(DB_CONNECTION)\n",
    "    print(\"‚úÖ Parsed JDBC connection successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to parse DB connection string: {e}\")\n",
    "    # Abort early: downstream cells depend on this\n",
    "    raise\n",
    "\n",
    "# Shared options dict for JDBC reads\n",
    "JDBC_OPTIONS_BASE = {\n",
    "    \"url\": JDBC_URL,\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    # Potential tuning knobs (commented for workshop):\n",
    "    # \"fetchsize\": 10000,\n",
    "    # Partitioning options can be added for large tables.\n",
    "}\n",
    "\n",
    "def safe_load_table(table_name: str):\n",
    "    \"\"\"Load a table via Spark JDBC with error handling.\n",
    "    Returns DataFrame or empty DataFrame with schema=None on failure.\"\"\"\n",
    "    try:\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", JDBC_OPTIONS_BASE[\"url\"]) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", JDBC_OPTIONS_BASE[\"user\"]) \\\n",
    "            .option(\"password\", JDBC_OPTIONS_BASE[\"password\"]) \\\n",
    "            .option(\"driver\", JDBC_OPTIONS_BASE[\"driver\"]) \\\n",
    "            .load()\n",
    "        count = df.count()  # triggers load; acceptable here for confirmation\n",
    "        print(f\"‚úÖ Loaded {table_name} (rows={count})\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {table_name}: {e}\")\n",
    "        from pyspark.sql import DataFrame\n",
    "        # Return an empty DataFrame to allow pipeline to proceed gracefully\n",
    "        empty_rdd = spark.sparkContext.emptyRDD()\n",
    "        return spark.createDataFrame(empty_rdd, schema=StructType([]))\n",
    "\n",
    "# Load required base tables\n",
    "customers = safe_load_table(\"raw.customers\")\n",
    "orders = safe_load_table(\"raw.orders\")\n",
    "order_items = safe_load_table(\"raw.order_items\")\n",
    "products = safe_load_table(\"raw.products\")\n",
    "\n",
    "# Optional caching for reuse in multiple analyses\n",
    "for df_name in [\"customers\", \"orders\", \"order_items\", \"products\"]:\n",
    "    df_obj = globals()[df_name]\n",
    "    if df_obj.head(1):  # only cache if not empty\n",
    "        globals()[df_name] = df_obj.cache()\n",
    "        print(f\"üóÉÔ∏è Cached DataFrame: {df_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5d23",
   "metadata": {},
   "source": [
    "# üêõ Data Pipeline Debugging Exercise\n",
    "\n",
    "This notebook contains a data pipeline with **several bugs and performance issues**. Your task is to use GitHub Copilot to identify and fix them.\n",
    "\n",
    "## Your Mission:\n",
    "Use GitHub Copilot Chat to:\n",
    "1. Review the code and identify issues\n",
    "2. Understand what each section is trying to accomplish\n",
    "3. Fix bugs and optimize performance\n",
    "4. Add proper error handling and validation\n",
    "\n",
    "## Hints:\n",
    "- Try asking Copilot to review specific cells\n",
    "- Ask about performance optimization\n",
    "- Request explanations for suspicious code patterns\n",
    "- Use Copilot to suggest best practices\n",
    "\n",
    "Good luck! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c343c",
   "metadata": {},
   "source": [
    "## Step 1: Load Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0bb102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 customers\n",
      "Loaded 5000 orders\n",
      "Loaded 15072 order items\n",
      "Loaded 200 products\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Data from Database (Refactored with error handling)\n",
    "# Using helper and safe_load_table defined earlier.\n",
    "\n",
    "required_tables = [\n",
    "    \"raw.customers\", \"raw.orders\", \"raw.order_items\", \"raw.products\"\n",
    "]\n",
    "\n",
    "loaded = {}\n",
    "for t in required_tables:\n",
    "    df = safe_load_table(t)\n",
    "    loaded[t.split('.')[-1]] = df\n",
    "\n",
    "customers = loaded[\"customers\"]\n",
    "orders = loaded[\"orders\"]\n",
    "order_items = loaded[\"order_items\"]\n",
    "products = loaded[\"products\"]\n",
    "\n",
    "# Validate minimal presence of rows (demo criteria)\n",
    "if customers.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è customers table is empty or failed to load; downstream analyses may be limited.\")\n",
    "if orders.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è orders table is empty or failed to load; trend and RFM analysis will be skipped.\")\n",
    "if order_items.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è order_items table is empty; revenue analysis will be skipped.\")\n",
    "if products.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è products table is empty; product enrichment will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfbe40",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Product Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a056749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join order items with product information\n",
    "product_sales = order_items.join(\n",
    "    products,\n",
    "    order_items.order_id == products.product_id,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Calculate line total with discount applied\n",
    "product_sales = product_sales.withColumn(\n",
    "    \"line_total\",\n",
    "    F.col(\"quantity\") * F.col(\"unit_price\") * (1 - F.col(\"discount_percent\"))\n",
    ")\n",
    "\n",
    "# Aggregate revenue by product\n",
    "revenue_by_product = product_sales.groupBy(\"product_id\", \"product_name\", \"category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"line_total\").alias(\"total_revenue\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.count(\"order_item_id\").alias(\"num_orders\")\n",
    "    )\n",
    "\n",
    "print(\"Top 10 Products by Revenue:\")\n",
    "revenue_by_product.orderBy(F.desc(\"total_revenue\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25befea5",
   "metadata": {},
   "source": [
    "## Step 3: Customer Segmentation (RFM Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde59f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Calculate RFM metrics for customer segmentation\n",
    "reference_date = datetime(2024, 1, 1)\n",
    "\n",
    "# Join customers with their orders\n",
    "customer_orders = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Calculate RFM metrics\n",
    "rfm = customer_orders.groupBy(\"customer_id\", \"customer_name\", \"country\") \\\n",
    "    .agg(\n",
    "        F.datediff(F.lit(reference_date), F.max(\"order_date\")).alias(\"recency\"),\n",
    "        F.count(\"order_id\").alias(\"frequency\"),\n",
    "        F.sum(\"total_amount\").alias(\"monetary\")\n",
    "    )\n",
    "\n",
    "# Score recency (1-5 scale)\n",
    "rfm = rfm.withColumn(\n",
    "    \"r_score\",\n",
    "    F.when(F.col(\"recency\") < 30, 1)\n",
    "     .when(F.col(\"recency\") < 60, 2)\n",
    "     .when(F.col(\"recency\") < 90, 3)\n",
    "     .when(F.col(\"recency\") < 180, 4)\n",
    "     .otherwise(5)\n",
    ").withColumn(\n",
    "    \"f_score\",\n",
    "    F.when(F.col(\"frequency\") >= 10, 5)\n",
    "     .when(F.col(\"frequency\") >= 5, 4)\n",
    "     .when(F.col(\"frequency\") >= 3, 3)\n",
    "     .when(F.col(\"frequency\") >= 2, 2)\n",
    "     .otherwise(1)\n",
    ").withColumn(\n",
    "    \"m_score\",\n",
    "    F.when(F.col(\"monetary\") >= 10000, 5)\n",
    "     .when(F.col(\"monetary\") >= 5000, 4)\n",
    "     .when(F.col(\"monetary\") >= 2000, 3)\n",
    "     .when(F.col(\"monetary\") >= 1000, 2)\n",
    "     .otherwise(1)\n",
    ")\n",
    "\n",
    "# Calculate overall RFM score\n",
    "rfm = rfm.withColumn(\n",
    "    \"rfm_score\",\n",
    "    F.col(\"r_score\") + F.col(\"f_score\") + F.col(\"m_score\")\n",
    ")\n",
    "\n",
    "print(\"Customer Segmentation Results:\")\n",
    "rfm.orderBy(F.desc(\"rfm_score\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4ba11",
   "metadata": {},
   "source": [
    "## Step 4: Sales Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe04b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly sales trends\n",
    "monthly_sales = orders.withColumn(\n",
    "    \"month\",\n",
    "    F.date_format(\"order_date\", \"yyyy-MM\")\n",
    ")\n",
    "\n",
    "# Aggregate sales by month\n",
    "monthly_sales = monthly_sales.groupBy(\"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_id\").alias(\"unique_customers\"),\n",
    "        F.sum(\"total_amount\").alias(\"revenue\"),\n",
    "        F.sum(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "print(\"Monthly Sales Trends:\")\n",
    "monthly_sales.orderBy(\"month\").show(12)\n",
    "\n",
    "# Calculate month-over-month growth rate\n",
    "windowSpec = Window.orderBy(\"month\")\n",
    "monthly_sales = monthly_sales.withColumn(\n",
    "    \"prev_month_revenue\",\n",
    "    F.lead(\"revenue\").over(windowSpec)\n",
    ")\n",
    "\n",
    "monthly_sales = monthly_sales.withColumn(\n",
    "    \"growth_rate\",\n",
    "    ((F.col(\"revenue\") - F.col(\"prev_month_revenue\")) / F.col(\"prev_month_revenue\") * 100)\n",
    ")\n",
    "\n",
    "print(\"\\nMonthly Growth Rates:\")\n",
    "monthly_sales.select(\"month\", \"revenue\", \"prev_month_revenue\", \"growth_rate\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
