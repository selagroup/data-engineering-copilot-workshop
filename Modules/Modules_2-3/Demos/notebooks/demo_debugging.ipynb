{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22a8efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Spark session initialized successfully!\n",
      "Spark version: 3.4.1\n",
      "Running on: win32\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install pyspark python-dotenv\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection string\n",
    "DB_CONNECTION = os.getenv('DB_CONNECTION_STRING', \n",
    "                          'postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db')\n",
    "\n",
    "# IMPORTANT: Set up Hadoop for Windows BEFORE creating Spark session\n",
    "if sys.platform.startswith('win'):\n",
    "    # Create a minimal Hadoop directory structure for Windows\n",
    "    hadoop_home = os.path.join(os.path.expanduser('~'), '.hadoop')\n",
    "    os.makedirs(hadoop_home, exist_ok=True)\n",
    "    os.makedirs(os.path.join(hadoop_home, 'bin'), exist_ok=True)\n",
    "    os.environ['HADOOP_HOME'] = hadoop_home\n",
    "    \n",
    "    # Download winutils.exe if not present (required for Windows)\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(\"⚠️ winutils.exe not found. Downloading...\")\n",
    "        import urllib.request\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                'https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/winutils.exe',\n",
    "                winutils_path\n",
    "            )\n",
    "            print(\"✅ winutils.exe downloaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not download winutils.exe automatically: {e}\")\n",
    "            print(\"Please download manually from: https://github.com/steveloughran/winutils\")\n",
    "\n",
    "# Initialize Spark Session with PostgreSQL driver\n",
    "# Note: The driver will be downloaded on first run, which may take a moment\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataPipelineDebugging\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Running on: {sys.platform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5d23",
   "metadata": {},
   "source": [
    "# 🐛 Data Pipeline Debugging Exercise\n",
    "\n",
    "This notebook contains a data pipeline with **several bugs and performance issues**. Your task is to use GitHub Copilot to identify and fix them.\n",
    "\n",
    "## Your Mission:\n",
    "Use GitHub Copilot Chat to:\n",
    "1. Review the code and identify issues\n",
    "2. Understand what each section is trying to accomplish\n",
    "3. Fix bugs and optimize performance\n",
    "4. Add proper error handling and validation\n",
    "\n",
    "## Hints:\n",
    "- Try asking Copilot to review specific cells\n",
    "- Ask about performance optimization\n",
    "- Request explanations for suspicious code patterns\n",
    "- Use Copilot to suggest best practices\n",
    "\n",
    "Good luck! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c343c",
   "metadata": {},
   "source": [
    "## Step 1: Load Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bb102",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The driver could not open a JDBC connection. Check the URL: postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 🐛 ISSUE #1: Performance - Loading entire tables with SELECT *\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This loads ALL columns even though we only need a few\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Also no predicate pushdown - filtering happens after loading all data\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load customers - inefficient!\u001b[39;00m\n\u001b[32m      6\u001b[39m customers = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjdbc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDB_CONNECTION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdbtable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw.customers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdriver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.postgresql.Driver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load orders - loading everything!\u001b[39;00m\n\u001b[32m     14\u001b[39m orders = spark.read \\\n\u001b[32m     15\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mjdbc\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     16\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m, DB_CONNECTION) \\\n\u001b[32m     17\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mdbtable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mraw.orders\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     18\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mdriver\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morg.postgresql.Driver\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     19\u001b[39m     .load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    171\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: requirement failed: The driver could not open a JDBC connection. Check the URL: postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db"
     ]
    }
   ],
   "source": [
    "# Load data from PostgreSQL database\n",
    "customers = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_CONNECTION) \\\n",
    "    .option(\"dbtable\", \"raw.customers\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "orders = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_CONNECTION) \\\n",
    "    .option(\"dbtable\", \"raw.orders\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "order_items = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_CONNECTION) \\\n",
    "    .option(\"dbtable\", \"raw.order_items\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "products = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", DB_CONNECTION) \\\n",
    "    .option(\"dbtable\", \"raw.products\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Loaded {customers.count()} customers\")\n",
    "print(f\"Loaded {orders.count()} orders\")\n",
    "print(f\"Loaded {order_items.count()} order items\")\n",
    "print(f\"Loaded {products.count()} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfbe40",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Product Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a056749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join order items with product information\n",
    "product_sales = order_items.join(\n",
    "    products,\n",
    "    order_items.order_id == products.product_id,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Calculate line total with discount applied\n",
    "product_sales = product_sales.withColumn(\n",
    "    \"line_total\",\n",
    "    F.col(\"quantity\") * F.col(\"unit_price\") * (1 - F.col(\"discount_percent\"))\n",
    ")\n",
    "\n",
    "# Aggregate revenue by product\n",
    "revenue_by_product = product_sales.groupBy(\"product_id\", \"product_name\", \"category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"line_total\").alias(\"total_revenue\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.count(\"order_item_id\").alias(\"num_orders\")\n",
    "    )\n",
    "\n",
    "print(\"Top 10 Products by Revenue:\")\n",
    "revenue_by_product.orderBy(F.desc(\"total_revenue\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25befea5",
   "metadata": {},
   "source": [
    "## Step 3: Customer Segmentation (RFM Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde59f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Calculate RFM metrics for customer segmentation\n",
    "reference_date = datetime(2024, 1, 1)\n",
    "\n",
    "# Join customers with their orders\n",
    "customer_orders = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Calculate RFM metrics\n",
    "rfm = customer_orders.groupBy(\"customer_id\", \"customer_name\", \"country\") \\\n",
    "    .agg(\n",
    "        F.datediff(F.lit(reference_date), F.max(\"order_date\")).alias(\"recency\"),\n",
    "        F.count(\"order_id\").alias(\"frequency\"),\n",
    "        F.sum(\"total_amount\").alias(\"monetary\")\n",
    "    )\n",
    "\n",
    "# Score recency (1-5 scale)\n",
    "rfm = rfm.withColumn(\n",
    "    \"r_score\",\n",
    "    F.when(F.col(\"recency\") < 30, 1)\n",
    "     .when(F.col(\"recency\") < 60, 2)\n",
    "     .when(F.col(\"recency\") < 90, 3)\n",
    "     .when(F.col(\"recency\") < 180, 4)\n",
    "     .otherwise(5)\n",
    ").withColumn(\n",
    "    \"f_score\",\n",
    "    F.when(F.col(\"frequency\") >= 10, 5)\n",
    "     .when(F.col(\"frequency\") >= 5, 4)\n",
    "     .when(F.col(\"frequency\") >= 3, 3)\n",
    "     .when(F.col(\"frequency\") >= 2, 2)\n",
    "     .otherwise(1)\n",
    ").withColumn(\n",
    "    \"m_score\",\n",
    "    F.when(F.col(\"monetary\") >= 10000, 5)\n",
    "     .when(F.col(\"monetary\") >= 5000, 4)\n",
    "     .when(F.col(\"monetary\") >= 2000, 3)\n",
    "     .when(F.col(\"monetary\") >= 1000, 2)\n",
    "     .otherwise(1)\n",
    ")\n",
    "\n",
    "# Calculate overall RFM score\n",
    "rfm = rfm.withColumn(\n",
    "    \"rfm_score\",\n",
    "    F.col(\"r_score\") + F.col(\"f_score\") + F.col(\"m_score\")\n",
    ")\n",
    "\n",
    "print(\"Customer Segmentation Results:\")\n",
    "rfm.orderBy(F.desc(\"rfm_score\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4ba11",
   "metadata": {},
   "source": [
    "## Step 4: Sales Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe04b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly sales trends\n",
    "monthly_sales = orders.withColumn(\n",
    "    \"month\",\n",
    "    F.date_format(\"order_date\", \"yyyy-MM\")\n",
    ")\n",
    "\n",
    "# Aggregate sales by month\n",
    "monthly_sales = monthly_sales.groupBy(\"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_id\").alias(\"unique_customers\"),\n",
    "        F.sum(\"total_amount\").alias(\"revenue\"),\n",
    "        F.sum(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "print(\"Monthly Sales Trends:\")\n",
    "monthly_sales.orderBy(\"month\").show(12)\n",
    "\n",
    "# Calculate month-over-month growth rate\n",
    "windowSpec = Window.orderBy(\"month\")\n",
    "monthly_sales = monthly_sales.withColumn(\n",
    "    \"prev_month_revenue\",\n",
    "    F.lead(\"revenue\").over(windowSpec)\n",
    ")\n",
    "\n",
    "monthly_sales = monthly_sales.withColumn(\n",
    "    \"growth_rate\",\n",
    "    ((F.col(\"revenue\") - F.col(\"prev_month_revenue\")) / F.col(\"prev_month_revenue\") * 100)\n",
    ")\n",
    "\n",
    "print(\"\\nMonthly Growth Rates:\")\n",
    "monthly_sales.select(\"month\", \"revenue\", \"prev_month_revenue\", \"growth_rate\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
