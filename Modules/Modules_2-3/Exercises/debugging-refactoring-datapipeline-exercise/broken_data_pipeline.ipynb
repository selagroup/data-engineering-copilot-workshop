{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22a8efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Spark session initialized successfully!\n",
      "Spark version: 3.4.1\n",
      "Running on: win32\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install pyspark python-dotenv\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection string\n",
    "DB_CONNECTION = os.getenv('DB_CONNECTION_STRING', \n",
    "                          'postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db')\n",
    "\n",
    "# IMPORTANT: Set up Hadoop for Windows BEFORE creating Spark session\n",
    "if sys.platform.startswith('win'):\n",
    "    # Create a minimal Hadoop directory structure for Windows\n",
    "    hadoop_home = os.path.join(os.path.expanduser('~'), '.hadoop')\n",
    "    os.makedirs(hadoop_home, exist_ok=True)\n",
    "    os.makedirs(os.path.join(hadoop_home, 'bin'), exist_ok=True)\n",
    "    os.environ['HADOOP_HOME'] = hadoop_home\n",
    "    \n",
    "    # Download winutils.exe if not present (required for Windows)\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(\"âš ï¸ winutils.exe not found. Downloading...\")\n",
    "        import urllib.request\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                'https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/winutils.exe',\n",
    "                winutils_path\n",
    "            )\n",
    "            print(\"âœ… winutils.exe downloaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not download winutils.exe automatically: {e}\")\n",
    "            print(\"Please download manually from: https://github.com/steveloughran/winutils\")\n",
    "\n",
    "# Initialize Spark Session with PostgreSQL driver\n",
    "# Note: The driver will be downloaded on first run, which may take a moment\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataPipelineDebugging\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Running on: {sys.platform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5d23",
   "metadata": {},
   "source": [
    "# ðŸ› Advanced Data Pipeline Debugging Exercise\n",
    "\n",
    "This notebook contains a more complex e-commerce analytics pipeline with **multiple subtle bugs, logic errors, and performance issues**. Your task is to use GitHub Copilot to identify and fix them.\n",
    "\n",
    "## Business Context:\n",
    "You're building an analytics platform that includes:\n",
    "- **Cohort Analysis**: Track customer retention over time\n",
    "- **Product Affinity**: Identify products frequently bought together\n",
    "- **Customer Lifetime Value (CLV)**: Calculate customer value metrics\n",
    "- **Sales Dashboard**: Daily performance metrics with trends\n",
    "- **Category Analysis**: Product category performance\n",
    "\n",
    "## Your Mission:\n",
    "Use GitHub Copilot Chat to:\n",
    "1. Review each section and identify bugs\n",
    "2. Fix logic errors in complex calculations\n",
    "3. Optimize performance bottlenecks\n",
    "4. Add proper error handling and validation\n",
    "5. Handle edge cases and null values\n",
    "\n",
    "\n",
    "## Tips:\n",
    "- Read the DEBUGGING_GUIDE.md for detailed explanations\n",
    "- Ask Copilot to explain the business logic first\n",
    "- Test your fixes incrementally\n",
    "- Think about edge cases (new customers, cancelled orders, etc.)\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c343c",
   "metadata": {},
   "source": [
    "## Step 1: Load Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bb102",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The driver could not open a JDBC connection. Check the URL: postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ðŸ› ISSUE #1: Performance - Loading entire tables with SELECT *\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This loads ALL columns even though we only need a few\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Also no predicate pushdown - filtering happens after loading all data\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load customers - inefficient!\u001b[39;00m\n\u001b[32m      6\u001b[39m customers = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjdbc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDB_CONNECTION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdbtable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw.customers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdriver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.postgresql.Driver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load orders - loading everything!\u001b[39;00m\n\u001b[32m     14\u001b[39m orders = spark.read \\\n\u001b[32m     15\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mjdbc\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     16\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m, DB_CONNECTION) \\\n\u001b[32m     17\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mdbtable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mraw.orders\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     18\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mdriver\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morg.postgresql.Driver\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     19\u001b[39m     .load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roeel\\projects\\sela\\data-engineering-copilot-workshop\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    171\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: requirement failed: The driver could not open a JDBC connection. Check the URL: postgresql://postgressadmin:wf**F!$3dGdf14@copilot-workshop-db.postgres.database.azure.com:5432/workshop_db"
     ]
    }
   ],
   "source": [
    "# Load data from PostgreSQL database\n",
    "# Parse connection string properly for JDBC\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed = urlparse(DB_CONNECTION)\n",
    "jdbc_url = f\"jdbc:postgresql://{parsed.hostname}:{parsed.port}{parsed.path}?ssl=true&sslmode=require\"\n",
    "username = parsed.username\n",
    "password = parsed.password\n",
    "\n",
    "# Load with proper authentication\n",
    "customers = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.customers\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "orders = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.orders\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "order_items = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.order_items\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "products = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"raw.products\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Loaded {customers.count()} customers\")\n",
    "print(f\"Loaded {orders.count()} orders\")\n",
    "print(f\"Loaded {order_items.count()} order items\")\n",
    "print(f\"Loaded {products.count()} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf57be2",
   "metadata": {},
   "source": [
    "## Step 2: Customer Cohort Analysis\n",
    "Track customer retention by analyzing cohorts based on their first purchase month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18052a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: Uses order_date instead of first order date for cohort assignment\n",
    "# This will assign customers to multiple cohorts!\n",
    "cohort_data = orders.withColumn(\n",
    "    \"cohort_month\",\n",
    "    F.date_trunc(\"month\", F.col(\"order_date\"))  # WRONG!\n",
    ").withColumn(\n",
    "    \"order_month\",\n",
    "    F.date_trunc(\"month\", F.col(\"order_date\"))\n",
    ")\n",
    "\n",
    "# Calculate period number (months since cohort)\n",
    "# BUG: Uses order_date instead of order_month\n",
    "cohort_data = cohort_data.withColumn(\n",
    "    \"period_number\",\n",
    "    F.months_between(F.col(\"order_date\"), F.col(\"cohort_month\")).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Count customers in each cohort period\n",
    "# BUG: Counts orders instead of unique customers\n",
    "cohort_counts = cohort_data.groupBy(\"cohort_month\", \"period_number\").agg(\n",
    "    F.count(\"order_id\").alias(\"customers\")\n",
    ")\n",
    "\n",
    "# Get cohort sizes (period 0)\n",
    "cohort_sizes = cohort_counts.filter(F.col(\"period_number\") == 0) \\\n",
    "    .select(\n",
    "        F.col(\"cohort_month\"),\n",
    "        F.col(\"customers\").alias(\"cohort_size\")\n",
    "    )\n",
    "\n",
    "# Calculate retention rates\n",
    "# BUG: No null handling, division by zero possible\n",
    "cohort_retention = cohort_counts.join(\n",
    "    cohort_sizes,\n",
    "    \"cohort_month\",\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"retention_rate\",\n",
    "    F.col(\"customers\") / F.col(\"cohort_size\") * 100\n",
    ")\n",
    "\n",
    "print(\"Cohort Retention Analysis:\")\n",
    "cohort_retention.orderBy(\"cohort_month\", \"period_number\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046f9e7",
   "metadata": {},
   "source": [
    "## Step 3: Product Affinity Analysis (Market Basket)\n",
    "Identify products that are frequently purchased together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e69855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get product information for each order\n",
    "order_products = order_items.join(\n",
    "    products,\n",
    "    \"product_id\",\n",
    "    \"inner\"\n",
    ").select(\"order_id\", \"product_id\", \"product_name\")\n",
    "\n",
    "# BUG: Self-join without preventing duplicates and self-pairs\n",
    "# This creates (A,A), (A,B), and (B,A) - massive data duplication!\n",
    "product_pairs = order_products.alias(\"a\").join(\n",
    "    order_products.alias(\"b\"),\n",
    "    F.col(\"a.order_id\") == F.col(\"b.order_id\"),  # Missing constraint!\n",
    "    \"inner\"\n",
    ").select(\n",
    "    F.col(\"a.product_id\").alias(\"product_a\"),\n",
    "    F.col(\"a.product_name\").alias(\"product_a_name\"),\n",
    "    F.col(\"b.product_id\").alias(\"product_b\"),\n",
    "    F.col(\"b.product_name\").alias(\"product_b_name\"),\n",
    "    F.col(\"a.order_id\")\n",
    ")\n",
    "\n",
    "# Count pair occurrences\n",
    "pair_counts = product_pairs.groupBy(\"product_a\", \"product_a_name\", \"product_b\", \"product_b_name\").agg(\n",
    "    F.count(\"order_id\").alias(\"pair_count\")\n",
    ")\n",
    "\n",
    "# Count individual product occurrences\n",
    "product_counts = order_products.groupBy(\"product_id\", \"product_name\").agg(\n",
    "    F.count(\"order_id\").alias(\"product_count\")\n",
    ")\n",
    "\n",
    "# Calculate affinity metrics\n",
    "# BUG: Missing broadcast optimization for this join\n",
    "# BUG: Incomplete confidence calculation\n",
    "affinity_metrics = pair_counts \\\n",
    "    .join(\n",
    "        product_counts.select(\n",
    "            F.col(\"product_id\").alias(\"product_a\"),\n",
    "            F.col(\"product_count\").alias(\"product_a_count\")\n",
    "        ),\n",
    "        \"product_a\",\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .join(\n",
    "        product_counts.select(\n",
    "            F.col(\"product_id\").alias(\"product_b\"),\n",
    "            F.col(\"product_count\").alias(\"product_b_count\")\n",
    "        ),\n",
    "        \"product_b\",\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"confidence\",\n",
    "        F.col(\"pair_count\") / F.col(\"product_a_count\")  # Simplified, missing context\n",
    "    )\n",
    "\n",
    "print(\"Top 10 Product Affinities:\")\n",
    "affinity_metrics.orderBy(F.desc(\"pair_count\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d1a56",
   "metadata": {},
   "source": [
    "## Step 4: Customer Lifetime Value (CLV) Calculation\n",
    "Calculate the value each customer brings over their lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58071708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: Includes ALL orders regardless of status (cancelled, returned, etc.)\n",
    "customer_revenue = orders.groupBy(\"customer_id\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    F.count(\"order_id\").alias(\"order_count\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    ")\n",
    "\n",
    "# Calculate customer metrics\n",
    "# BUG: Uses first order date instead of customer join date\n",
    "# BUG: Integer division in some Spark versions\n",
    "customer_metrics = orders.groupBy(\"customer_id\").agg(\n",
    "    F.min(\"order_date\").alias(\"first_order\"),\n",
    "    F.max(\"order_date\").alias(\"last_order\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"total_orders\")\n",
    ").withColumn(\n",
    "    \"customer_lifespan_years\",\n",
    "    F.datediff(F.current_date(), F.col(\"first_order\")) / 365\n",
    ").withColumn(\n",
    "    \"purchase_frequency\",\n",
    "    F.col(\"total_orders\") / F.col(\"customer_lifespan_years\")\n",
    ")\n",
    "\n",
    "# Calculate CLV\n",
    "# BUG: Inner join loses customers without completed orders\n",
    "# BUG: Doesn't handle edge cases (lifespan = 0 for new customers)\n",
    "clv = customer_revenue.join(\n",
    "    customer_metrics,\n",
    "    \"customer_id\",\n",
    "    \"inner\"\n",
    ").withColumn(\n",
    "    \"customer_lifetime_value\",\n",
    "    F.col(\"avg_order_value\") * F.col(\"purchase_frequency\") * F.col(\"customer_lifespan_years\")\n",
    ")\n",
    "\n",
    "print(\"Top 10 Customers by CLV:\")\n",
    "clv.orderBy(F.desc(\"customer_lifetime_value\")).show(10)\n",
    "\n",
    "print(\"\\nCLV Statistics:\")\n",
    "clv.select(\n",
    "    F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "    F.stddev(\"customer_lifetime_value\").alias(\"stddev_clv\"),\n",
    "    F.min(\"customer_lifetime_value\").alias(\"min_clv\"),\n",
    "    F.max(\"customer_lifetime_value\").alias(\"max_clv\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7637f31",
   "metadata": {},
   "source": [
    "## Step 5: Sales Performance Dashboard\n",
    "Daily sales metrics with moving averages and cumulative totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: Multiple separate aggregations - should combine for performance\n",
    "daily_revenue = orders.groupBy(\"order_date\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "daily_orders = orders.groupBy(\"order_date\").agg(\n",
    "    F.count(\"order_id\").alias(\"total_orders\")\n",
    ")\n",
    "\n",
    "daily_customers = orders.groupBy(\"order_date\").agg(\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    ")\n",
    "\n",
    "# Combine metrics\n",
    "daily_sales = daily_revenue.join(daily_orders, \"order_date\", \"outer\") \\\n",
    "    .join(daily_customers, \"order_date\", \"outer\") \\\n",
    "    .withColumn(\"avg_order_value\", F.col(\"total_revenue\") / F.col(\"total_orders\"))\n",
    "\n",
    "# BUG: Wrong window specification for 7-day moving average\n",
    "# rowsBetween(-6, 0) assumes continuous dates, which may not exist\n",
    "windowSpec = Window.orderBy(\"order_date\").rowsBetween(-6, 0)\n",
    "\n",
    "daily_sales = daily_sales.withColumn(\n",
    "    \"moving_avg_7d\",\n",
    "    F.avg(\"total_revenue\").over(windowSpec)\n",
    ")\n",
    "\n",
    "# BUG: Cumulative sum without year partitioning\n",
    "# This continues across years instead of resetting\n",
    "windowSpec = Window.orderBy(\"order_date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "daily_sales = daily_sales.withColumn(\n",
    "    \"ytd_revenue\",\n",
    "    F.sum(\"total_revenue\").over(windowSpec)\n",
    ")\n",
    "\n",
    "print(\"Daily Sales Performance:\")\n",
    "daily_sales.orderBy(F.desc(\"order_date\")).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ab2bc",
   "metadata": {},
   "source": [
    "## Step 6: Category Performance Analysis\n",
    "Analyze sales performance by product category with rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: Direct join between products and order_items causes double counting\n",
    "# Should aggregate order_items first\n",
    "category_sales = products.join(\n",
    "    order_items,\n",
    "    \"product_id\",\n",
    "    \"inner\"\n",
    ").withColumn(\n",
    "    \"line_total\",\n",
    "    F.col(\"quantity\") * F.col(\"unit_price\") * (1 - F.coalesce(F.col(\"discount_percent\"), F.lit(0)) / 100)\n",
    ")\n",
    "\n",
    "# Aggregate by category\n",
    "category_performance = category_sales.groupBy(\"category\").agg(\n",
    "    F.sum(\"line_total\").alias(\"total_revenue\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    F.countDistinct(\"product_id\").alias(\"unique_products\")\n",
    ").withColumn(\n",
    "    \"revenue_per_product\",\n",
    "    F.col(\"total_revenue\") / F.col(\"unique_products\")\n",
    ")\n",
    "\n",
    "print(\"Category Performance:\")\n",
    "category_performance.orderBy(F.desc(\"total_revenue\")).show()\n",
    "\n",
    "# Product ranking within categories\n",
    "# BUG: Ranks globally instead of within each category\n",
    "product_revenue = order_items.groupBy(\"product_id\").agg(\n",
    "    F.sum(F.col(\"quantity\") * F.col(\"unit_price\") * \n",
    "          (1 - F.coalesce(F.col(\"discount_percent\"), F.lit(0)) / 100)).alias(\"revenue\")\n",
    ")\n",
    "\n",
    "product_with_category = products.join(product_revenue, \"product_id\", \"inner\")\n",
    "\n",
    "# Missing partition by category in window!\n",
    "windowSpec = Window.orderBy(F.desc(\"revenue\"))\n",
    "\n",
    "top_products = product_with_category.withColumn(\n",
    "    \"rank\",\n",
    "    F.rank().over(windowSpec)\n",
    ").filter(F.col(\"rank\") <= 5)\n",
    "\n",
    "print(\"\\nTop 5 Products by Revenue (should be per category!):\")\n",
    "top_products.select(\"category\", \"product_name\", \"revenue\", \"rank\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8f69a",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "If you've made it this far, you've encountered numerous bugs! ðŸ›\n",
    "\n",
    "### Key Issues Found:\n",
    "1. **Cohort Analysis**: Wrong cohort assignment and period calculation\n",
    "2. **Product Affinity**: Duplicate pairs and missing constraints\n",
    "3. **CLV Calculation**: Invalid order inclusion and edge case handling\n",
    "4. **Sales Dashboard**: Window function errors and performance issues\n",
    "5. **Category Analysis**: Double counting and missing partitioning\n",
    "\n",
    "### How to Debug:\n",
    "1. Use GitHub Copilot Chat to review each cell\n",
    "2. Ask Copilot to explain what the code SHOULD do\n",
    "3. Compare with what it ACTUALLY does\n",
    "4. Use Copilot to generate fixes\n",
    "5. Test incrementally\n",
    "\n",
    "### Resources:\n",
    "- See `DEBUGGING_GUIDE.md` for detailed solutions\n",
    "- Ask Copilot: \"What's wrong with this cohort analysis?\"\n",
    "- Ask Copilot: \"How can I optimize this self-join?\"\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
